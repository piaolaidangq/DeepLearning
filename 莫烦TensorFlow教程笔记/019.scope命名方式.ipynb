{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.name_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1:0 [1.]\n",
      "a_name_scope/var2:0 [2.]\n",
      "a_name_scope/var2_1:0 [2.1]\n",
      "a_name_scope/var2_2:0 [2.2]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session() \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.name_scope('a_name_scope'):\n",
    "    \n",
    "    initializer = tf.constant_initializer(value=1)\n",
    "    var1 = tf.get_variable(name='var1', shape=[1], dtype=tf.float32, initializer=initializer)   # 创建变量方式1\n",
    "    \n",
    "    var2 = tf.Variable(name='var2', initial_value=[2], dtype=tf.float32)                        # 创建变量方式2\n",
    "    var21 = tf.Variable(name='var2', initial_value=[2.1], dtype=tf.float32)\n",
    "    var22 = tf.Variable(name='var2', initial_value=[2.2], dtype=tf.float32)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(var1.name, sess.run(var1))\n",
    "    print(var2.name, sess.run(var2))\n",
    "    print(var21.name, sess.run(var21))\n",
    "    print(var22.name, sess.run(var22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.variable_scope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_variable_scope/var3:0 [3.]\n",
      "a_variable_scope/var4:0 [4.]\n",
      "a_variable_scope/var4_1:0 [4.]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session() \n",
    "\n",
    "with tf.variable_scope('a_variable_scope') as scope:\n",
    "    \n",
    "    initializer = tf.constant_initializer(value=3)\n",
    "    var3 = tf.get_variable(name='var3', shape=[1], dtype=tf.float32, initializer=initializer)   # 采用 tf.variable_scope 时，变量名前有了‘a_variable_scope/’\n",
    "    \n",
    "    var4 = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)                        \n",
    "    var4_reuse = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)                  # 不能重复调用\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(var3.name, sess.run(var3))\n",
    "    print(var4.name, sess.run(var4))\n",
    "    print(var4_reuse.name, sess.run(var4_reuse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重复调用变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_variable_scope/var3:0 [3.]\n",
      "a_variable_scope/var3:0 [3.]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session() \n",
    "\n",
    "with tf.variable_scope('a_variable_scope') as scope:    # 调用 tf.variable_scope()\n",
    "    \n",
    "    initializer = tf.constant_initializer(value=3)\n",
    "    var3 = tf.get_variable(name='var3', shape=[1], dtype=tf.float32, initializer=initializer) \n",
    "    \n",
    "    scope.reuse_variables()                        # 声明其后变量重复调用\n",
    "    var3_reuse = tf.get_variable(name='var3')      # 重复调用\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print(var3.name, sess.run(var3))\n",
    "    print(var3_reuse.name, sess.run(var3_reuse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn/input_layer/weights:0\n",
      "rnn/input_layer/weights:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "class TrainConfig:\n",
    "    batch_size = 20\n",
    "    time_steps = 20\n",
    "    input_size = 10\n",
    "    output_size = 2\n",
    "    cell_size = 11\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "class TestConfig(TrainConfig):\n",
    "    time_steps = 1               # Test 中改变 time_step\n",
    "    \n",
    "class RNN:\n",
    "    def __init__(self, config):\n",
    "        self._batch_size = config.batch_size\n",
    "        self._time_steps = config.time_steps\n",
    "        self._input_size = config.input_size\n",
    "        self._output_size = config.output_size\n",
    "        self._cell_size = config.cell_size\n",
    "        self._lr = config.learning_rate\n",
    "        self._built_RNN()\n",
    "        \n",
    "    def _built_RNN(self):\n",
    "        with tf.variable_scope('inputs'):\n",
    "            self._xs = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._input_size], name='xs')\n",
    "            self._ys = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._output_size], name='ys')\n",
    "        with tf.name_scope('RNN'):\n",
    "            with tf.variable_scope('input_layer'):\n",
    "                l_in_x = tf.reshape(self._xs, [-1, self._input_size], name='2_2D')   # (batch*n_step, in_size)\n",
    "                Wi = self._weight_variable([self._input_size, self._cell_size])      # Ws(in_size, cell_size)\n",
    "                print(Wi.name)\n",
    "                bi = self._bias_variable([self._cell_size,])                         # bs(cell_size, )\n",
    "                # l_in_y(batch*n_steps, cell_size)\n",
    "                with tf.name_scope('Wx_plus_b'):\n",
    "                    l_in_y = tf.matmul(l_in_x, Wi)+bi\n",
    "                l_in_y = tf.reshape(l_in_y, [-1, self._time_steps, self._cell_size], name='2_3D')                  \n",
    "                \n",
    "            with tf.variable_scope('lstm_cell'):\n",
    "                # cell = tf.nn.rnn_cell.BasicRNNCell(self._cell_size)\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(self._cell_size)\n",
    "                with tf.name_scope('initial_state'):\n",
    "                    self._cell_initial_state = cell.zero_state(self._batch_size, dtype=tf.float32)\n",
    "        \n",
    "                self.cell_outputs = []\n",
    "                cell_state = self._cell_initial_state\n",
    "                for t in range(self._time_steps):\n",
    "                    if t > 0:\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "                    cell_output, cell_state = cell(l_in_y[:, t, :], cell_state)\n",
    "                    self.cell_outputs.append(cell_output)\n",
    "                self._cell_final_state = cell_state\n",
    "            \n",
    "            with tf.variable_scope('output_layer'):\n",
    "                # cell_outputs_reshaped(BATCH*TIME_STEP, CELL_SIZE)\n",
    "                cell_outputs_reshaped = tf.reshape(tf.concat(self.cell_outputs, 1), [-1, self._cell_size])\n",
    "                Wo = self._weight_variable((self._cell_size, self._output_size))\n",
    "                bo = self._bias_variable((self._output_size,))\n",
    "                product = tf.matmul(cell_outputs_reshaped, Wo) + bo\n",
    "                # _pred shape(batch*time_step, output_size)\n",
    "                self._pred = tf.nn.relu(product)            # for displacement\n",
    "                \n",
    "        with tf.name_scope('cost'):\n",
    "            _pred = tf.reshape(self._pred, [self._batch_size, self._time_steps, self._output_size])\n",
    "            mse = self.ms_error(_pred, self._ys)\n",
    "            mse_ave_across_batch = tf.reduce_mean(mse, 0)\n",
    "            mse_sum_across_time = tf.reduce_sum(mse_ave_across_batch, 0)\n",
    "            self._cost = mse_sum_across_time\n",
    "            self._cost_ave_time = self._cost/self._time_steps\n",
    "                \n",
    "        with tf.variable_scope('train'):\n",
    "            self._lr = tf.convert_to_tensor(self._lr)\n",
    "            self.train_op = tf.train.AdamOptimizer(self._lr).minimize(self._cost)\n",
    "                \n",
    "    @staticmethod\n",
    "    def ms_error(y_target, y_pre):\n",
    "        return tf.square(tf.subtract(y_target, y_pre))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _weight_variable(shape, name='weights'):\n",
    "        initizlizer = tf.random_normal_initializer(mean=0., stddev=0.5,)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _bias_variable(shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    train_config = TrainConfig()\n",
    "    test_config = TestConfig()\n",
    "    \n",
    "    # 不采用方法，直接调用（报错）\n",
    "    # train_rnn1 = RNN(train_config)       # 第一次调用 weight，输出 input_layer/weights:0\n",
    "    # test_rnn1 = RNN(test_config)         # 第二次调用 weight，报错 ValueError: Variable input_layer/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n",
    "    \n",
    "    # 采用 tf.variable_scope() 解决重复调用问题（并未达到共享变量的作用，两次调用名称不同，为不同变量）\n",
    "    # with tf.variable_scope('train_rnn'):  \n",
    "    #     train_rnn1 = RNN(train_config)    # 第一次调用 weight，输出 train_rnn/input_layer/weights:0\n",
    "    # with tf.variable_scope('test_rnn'):\n",
    "    #     test_rnn1 = RNN(test_config)      # 第二次调用 weight，输出 test_rnn/input_layer/weights:0\n",
    "        \n",
    "    # 采用 tf.variable_scope() 和 scope.reuse_variables() 解决共享变量问题\n",
    "    with tf.variable_scope('rnn') as scope:\n",
    "        sess = tf.Session()\n",
    "        train_rnn2 = RNN(train_config)     # 训练参数                   输出 rnn/input_layer/weights:0\n",
    "        \n",
    "        scope.reuse_variables()            # 共享变量申明\n",
    "        test_rnn2 = RNN(test_config)       # 重复利用 train 中的参数    输出 rnn/input_layer/weights:0\n",
    "        sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.square()，对各元素求平方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  4.  9. 16.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "test = tf.Graph()\n",
    "with test.as_default():\n",
    "    X = tf.constant([1, 2, 3, 4], dtype=tf.float32, name=None)\n",
    "    Y = tf.square(X)         # 平方操作\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reduce_mean(input_tensor, axis=None, keep_dims=False, name=None)，指定维度求均值，默认求所有元素均值\n",
    "* input_tesor，输入张量\n",
    "* axis，指定轴\n",
    "* keep_dims，是否降维。设置 True，结果保持输入 Tensor 的形状（维度）；设置 False，为降维后的结果；默认 False\n",
    "* name, 名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "[2.5 3.5 4.5]\n",
      "[2. 5.]\n",
      "[[2.5 3.5 4.5]]\n",
      "[[2.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "test = tf.Graph()\n",
    "with test.as_default():\n",
    "    X = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32, name=None)\n",
    "    Y = tf.reduce_mean(X)                # 求所有元素均值\n",
    "    Y0 = tf.reduce_mean(X, axis=0)       # 求 0 维度均值，即列均值\n",
    "    Y1 = tf.reduce_mean(X, axis=1)       # 求 1 维度均值，即行均值\n",
    "    \n",
    "    Y0_ = tf.reduce_mean(X, axis=0, keep_dims=True)       # 求行均值，保持输入形状\n",
    "    Y1_ = tf.reduce_mean(X, axis=1, keep_dims=True)       # 求行均值，保持输入形状\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(Y))\n",
    "        print(sess.run(Y0))\n",
    "        print(sess.run(Y1))\n",
    "        print(sess.run(Y0_))\n",
    "        print(sess.run(Y1_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类似函数：\n",
    "* tf.reduce_sum ：计算tensor指定轴方向上的所有元素的累加和；\n",
    "* tf.reduce_max  :  计算tensor指定轴方向上的各个元素的最大值；\n",
    "* tf.reduce_all :  计算tensor指定轴方向上的各个元素的逻辑和（and运算）；\n",
    "* tf.reduce_any:  计算tensor指定轴方向上的各个元素的逻辑或（or运算）\n",
    "    \n",
    "**参考**：[csdn博客](https://blog.csdn.net/dcrmg/article/details/79797826)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reshape(tensor, shape, name=None)\n",
    "* 改变 tensor 的 shape\n",
    "* shape 设置为 -1 的位置，表示不用设置这一维的大小，函数自动进行计算（只能存在一个 -1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原Tensor:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "(3, 2):\n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "(1, 6):\n",
      " [[1. 2. 3. 4. 5. 6.]]\n",
      "(6, 1):\n",
      " [[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "test = tf.Graph()\n",
    "with test.as_default():\n",
    "    X = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32, name=None)\n",
    "    Y1 = tf.reshape(X, (3, 2))        # (2, 3) --> (3, 2)\n",
    "    Y2 = tf.reshape(X, (-1, 6))       # (2, 3) --> (1, 6)\n",
    "    Y3 = tf.reshape(X, (6, -1))       # (2, 3) --> (6, 1)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print('原Tensor:\\n', sess.run(X))\n",
    "        print('(3, 2):\\n', sess.run(Y1))\n",
    "        print('(1, 6):\\n', sess.run(Y2))\n",
    "        print('(6, 1):\\n', sess.run(Y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.concat([tensor1, tensor2, tensor3, ... ], axis)，拼接张量\n",
    "* axis，指定轴（维度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n",
      "[[ 1.  2.  3.  7.  8.  9.]\n",
      " [ 4.  5.  6. 10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "test = tf.Graph()\n",
    "with test.as_default():\n",
    "    t1 = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "    t2 = tf.constant([[7, 8, 9], [10, 11, 12]], dtype=tf.float32)\n",
    "    T0 = tf.concat([t1, t2], axis=0)      # 维度 0 处拼接，即列拼接\n",
    "    T1 = tf.concat([t1, t2], axis=1)      # 维度 1 处拼接，即行拼接\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(T0))\n",
    "        print(sess.run(T1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
